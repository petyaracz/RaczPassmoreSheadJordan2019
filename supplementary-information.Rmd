---
title: "Supplementary Information, Usage frequency and lexical class determine the evolution of kinship terms in Indo-European"
author: "RÃ¡cz, Passmore, Sheard, and Jordan"
output:
  pdf_document:
    latex_engine: xelatex
    toc: yes
  html_document:
    number_sections: yes
    toc: yes
    toc_float: yes
bibliography: bibliography.bib
header-includes:
    - \usepackage{caption}
---
\captionsetup[table]{labelformat=empty}
```{r include=FALSE, cache=FALSE}
options(Encoding="UTF-8")
knitr::opts_chunk$set(fig.width=5, fig.height=5, fig.path='figures/', eval=TRUE, echo=FALSE, warning = FALSE, message=FALSE, tidy=TRUE)

#devtools::install_github('SamPassmore/bayestraitr') # our R package to work with BayesTraits

library(tidyverse)
library(lme4)
library(effects)
library(broom)
library(kableExtra)
library(sjPlot)
library(bayestraitr)
library(purrr)
library(coda)
library(psych)

format = 'pandoc'
#setwd('~/Github/RaczPassmoreSheardJordan2019/')
```

# Kinship data

```{r}
corpora = read_csv('data/corpora-si.csv') # si versions of data files have -si preffix
sd = read_csv('data/supp-data-si.csv')
kinterms = sd$meaning %>% 
  unique %>% 
  sort %>% 
  as.character
nlang = sd$language %>% 
  unique %>% 
  length
ncorpora = sd$corpus %>% 
  unique %>% 
  length
nlangfreq = sd %>% 
  filter(!is.na(corpus)) %>% 
  select(language) %>%
  unique %>% 
  nrow
```

We collected kin terms from `r nlang` languages for the following relations: `r kinterms` (i.e.\ brother, daughter, father, mother, mother's brother, mother's sister, mother's sister's daughter, mother's sister's son, son, sister), collected from a combination of native speakers, ethnographies, and dictionaries in our ['Kinbank' database](https://excd.org/research-activities/kinbank/). Initially, the analyses used a broader set of kinship terms (e.g. FB, FZ, BW, ZH, i.e.\ father's brother, father's sister, brother's wife, sister's husband), however, we restricted the sample to form a comparable set of word frequencies. This is because most Indo-European languages do not have separate terms for e.g.\ MZD and MBD and separate terms for e.g.\ BW are exceedingly rare (both across languages as types and within languages as tokens). We have not included _husband_ and _wife_, because these are often synonymous with _man_ and _woman_, respectively.

Frequency data were collected from `r ncorpora` corpora in `r nlangfreq` languages in three corpus types: spoken, written, and web-crawled. The list of corpora is in Table S1 below.

```{r}
corpora %>% 
  mutate(ref = paste('[@', bibtex, ']', sep = '')) %>% 
  select(language,corpus.name,ref) %>%
  kable(format, col.names = c('Language', 'Corpus name', 'Reference'), caption = 'Table S1: Source of frequency data (source of words is Kinbank)') %>% 
  kable_styling(full_width = FALSE)
```

The analysis uses one term per language per kinterm. If it arises that a language has multiple words for a particular kinterm, we use which ever was more frequent in the corpus data. In the case where a language has multiple words for a single kinterm, and we do not have access to frequency data, we rely on expert judgement to select a term. 

## Supplementary data table

See the supplementary data csv file for the raw data used in this study.

#### Column description:

```{r echo = FALSE}
cn = colnames(sd)
cn = cn[!cn %in% c('word.type')]
cn_description = c(
  "Code used for kinterm, see TS1 for a description",
  "Common name for each language",
  "Words used in the analysis",
  "Cognates as determined by lingpy, used as a first pass",
  "Cognates reviewed and corrected by expert reviewers, used in analysis",
  "Frequency of each term in the given corpus",
  "Total size of the corpus",
  "Type of corpus used (either web, written, or spoken)",
  "Name of the corpus",
  "States",
  "Taxa label in phylogeny",
  "Glottocode",
  "Standard deviation for the global rate of replacement",
  "Mean global rate of replacement",
  "Source of term (see source SM)"
)

data.frame(cn, cn_description) %>%
  kable(col.names = c("Column name", "Description"), format = format,
        caption = "Table S2: Supplementary table column descriptions") %>%
  kable_styling(full_width = FALSE)
```

## Data summary

We estimate rate of replacement and compare it with frequency of use for ten types of kin relations. 
Here we use MB, MZ, MZS, and MZD as shorthand for broader terms (uncle, aunt, male cousin, and female cousin, respectively). This is because, as we note in the first section, commonly used terms in this set of languages do not distinguish terms according to the parent's gender.

Below is a table indicating the shorthand used for each kinterm, the number of languages for which we have data, and the number of states (or cognates) for that term. 

We collected terms for a superset of the languages from which frequency data is available in order to have a more robust estimate of rate of replacement for each term.

```{r echo = FALSE}
kc_order = c("F", "M", "S", "D", "B", "Z", "MB", "MZ", "MZS", "MZD")
sd$kincode = factor(sd$meaning, 
                       levels = kc_order)

t = sd %>%
  group_by(kincode) %>%
  summarise(language.count = length(unique(language)), states = length(unique(expert.cognate)))
t$description = c("Father", "Mother", "Son", "Daughter", "Brother", "Sister", 
                  "Mother's brother (uncle)", "Mother's sister (aunt)", 
                  "Mother's sister's daughter (female cousin)", "Mother's sisters son (male cousin)")
t = t[,c(1,4,2,3)]

kable(t, col.names = c("Kinterm", "Description", "No. languages", "States"),
      format, caption = "Table S3: Each kinterm used, with counts for the number of
      languages we have data for and the number of cognates across our sample") %>%
  kable_styling(full_width = FALSE)

min.lang = min(t$language.count)
max.lang = max(t$language.count)

```



## Frequency data

Below are bar graphs of term frequency by language, across corpora types (web, written, or spoken).

```{r echo = FALSE, fig.align='center', fig.width=10, fig.height=15}
sd$corpus.type = factor(sd$corpora.type, levels = c("web", "written", "spoken"))

# d2 = d %>%
#   group_by(kincode, language) %>%
#   arrange(desc(word.count)) %>%
#   slice(which.min(corpus.type))

sd2 = sd %>%
  group_by(kincode, language, corpus.type) %>%
  slice(which.max(word.count))

df_freq = subset(sd2, subset = sd2$kincode %in% kc_order)

distance = data.frame(kincode = unique(df_freq$kincode), 
                      distance = factor(c(1,1,1,1,1,1,2,2,3,3), levels = 1:3))

language_order = c(
  "English", "German", "Danish", "Dutch", "Swedish", "Norwegian", ## Germanic
  "Spanish", "Catalan", "Portuguese", "French", "Italian", "Romanian",  ## Latin
  "Czech", "Polish", "Serbian", "Russian", "Bulgarian", "Croatian",  ## Slavic 
  "Greek", "Albanian", "Icelandic"  ## Other
)

df_freq = left_join(df_freq, distance, by = "kincode")

df_freq$language = factor(df_freq$language, levels = language_order)
df_freq$kincode = factor(df_freq$kincode, levels = kc_order)
df_freq = df_freq %>%
  mutate(fp20m = (2 * 10^7) * (word.count / corpora.size))

#pdf('figures/supp-figure-1.pdf')
ggplot(data=df_freq %>% filter(corpus.type == "web")) + 
  geom_col(aes(x = kincode, y = log(fp20m), fill = factor(distance))) + 
  facet_wrap(~ language, dir="v", ncol = 2) + 
  theme(axis.title.x=element_blank(),
        axis.text.x = element_text(angle = -90, hjust = 0, size = 10),
        axis.ticks.x=element_blank(), 
        legend.position="none") +
  ylab("log(frequency per 20 million)") +
  ylim(0, 13) + 
  ggtitle('Figure S1: Web frequencies of terms across languages')
#dev.off()

ggplot(data=df_freq %>% filter(corpus.type == "written")) + 
  geom_col(aes(x = kincode, y = log(fp20m), fill = factor(distance))) + 
  facet_wrap(~ language, dir="v", ncol = 2) + 
  theme(axis.title.x=element_blank(),
        axis.text.x = element_text(angle = -90, hjust = 0, size = 10),
        axis.ticks.x=element_blank(), 
        legend.position="none") +
  ylab("log(frequency per 20 million)") +
  ylim(0, 13)  + 
  ggtitle('Figure S2: Written frequencies of terms across languages')

ggplot(data=df_freq %>% filter(corpus.type == "spoken")) + 
  geom_col(aes(x = kincode, y = log(fp20m), fill = factor(distance))) + 
  facet_wrap(~ language, dir="v", nrow = 6) + 
  theme(axis.title.x=element_blank(),
        axis.text.x = element_text(angle = -90, hjust = 0, size = 10),
        axis.ticks.x=element_blank(), 
        legend.position="none") +
  ylab("log(frequency per 20 million)") +
  ylim(0, 13) + 
  ggtitle('Figure S3: Spoken frequencies of terms across languages')
```

# Cognate data

We generated cognate classes using the Indo-European Etymological Dictionary [@buck], LingPy [@list_lingpy._2018], and a panel of volunteer experts, recruited on Linguist List (all faults remain ours). All terms were automatically transcribed into the Speech Assessment Methods Phonetic Alphabet (SAMPA) through LingPy's `uni2sampa` function. Cognates were automatically allocated using LingPy's `cluster` function. Using the cognate-coded Swadesh list, the list of core vocabulary terms (subset to those languages for which we also have kinterms), we tested the appropriateness of edit distance, SCA, and turchin algorithms, alongside Phonemic and Phonetic transcriptions for cognate detection in our data, following the code examples from [Lingpy.org](http://lingpy.org/examples.html). The F-score was highest for the edit-distance algorithm with a 0.4 threshold (see table S4) [@list_lingpy._2018]. We then manually adjusted the results, followed by expert review which resulted in minor changes. Automatic decisions and the corrections are available in the supplementary data file.

```{r results_table }
cognate_detection = read.csv('data/cognatedetection-results-si.csv') 

kable(cognate_detection, 
      format, caption = "Table S4: Precision, recall, & F-score for various LingPy cognate detection algorithms and their threshold settings.") %>%
  kable_styling(full_width = FALSE)
```


## Phylogeny

We used 1000 phylogenies from the most recent Bayesian posterior of Indo-European phylogenies [@bouckaert_mapping_2012]. Trees in the sample are rooted. Branch lengths are given in years and derived from statistical and historical calibration. The Indo-European posterior used has an approximate age of 8,700 years. Trees initially have 111 taxa, and these were pruned down for each kinterm dependent on available data. Counts for taxa for each kinterm can be found in table S3. By using a sample of likely phyloygenies and through using a Bayesian approach, we account for the phylogenetic uncertainty.

## Rates of change

Table S3 shows the number of languages and states used to estimate rate of change for each kinterm. Each language is linked to a taxon in the Indo-European phylogeny. Following the methods in @pagel_deep_2018 we use BayesTraits version 3.0.1 to implement a Bayesian MCMC approach to estimate the instantaneous global rate of change for each kin-term through Q-matrix normalisation. Probabilities of frequency were scaled to represent the empirical frequencies. We used a stepping-stone sampler, using 100 stones for 1000 iterations each. MCMC chains ran for a total of 10,010,000 iterations, with a burn-in of 10,000, sampling every 1000 iterations. This left a posterior sample of 10,000 iterations, which is approximately 10 samples per tree. To make the rates comparable to Pagel et al., we scale instantaneous rates to change per 10,000 years.

Each analysis was run 3 times to ensure the MCMC chain converged. Tables S5 - S14 display the marginal log-likelihood for each MCMC run, the mean global rate of change for each run, and the average across the three runs. Each table is labelled by kin code. For each kinterm, we also used the Gelman-Rubin diagnostic test for convergence [@gelman1992inference]. This tests for MCMC convergence between multiple chains by analysing the differences between them. By estimating a 'potential scale reduction factor', which when multiplying across chains would remove the differences, we can quantify the differences between chains (a scale reduction factor 1 indicating no change needed). A rule of thumb suggests a point estimates of less than 1.1 is sufficient to claim convergence, and ensuring upper limits are also around these limits.  

```{r mcmc.diagnostics, eval = TRUE, results='asis'}
# get diagnostics and averages
# harmonic SD
harmonic.sd <- function(x)sqrt((mean(1/x))^(-4)*var(1/x)/length(x))

log.files = list.files('data/bayestraits-output/', pattern = "*.Log.txt", 
                       full.names = TRUE)
stone.files = list.files('data/bayestraits-output/', pattern = "*.Stones.txt", 
                         full.names = TRUE)

fs = log.files %>% 
  basename() %>%
  str_extract('[A-Z]{1,3}') %>% 
  unique() 

titles = fs %>% 
  paste0("Table S", 5:14, ": ", .)

gelman_diag = list()
average_result = list()
for(i in seq_along(titles)){
  idx = str_detect(
    log.files, paste0("_", fs[i], "_")
  )
  
  logs = map(log.files[idx], bt_read.log)
  stones = map(stone.files[idx], bt_read.stones)
  
  c123 = mcmc.list(as.mcmc(logs[[1]]$Lh),
                  as.mcmc(logs[[2]]$Lh),
                  as.mcmc(logs[[3]]$Lh))                            
  
  gelman_diag[[i]] = gelman.diag(c123)[[1]][1,]
  
  lh = map(stones, "marginal_likelihood") %>% 
    unlist(stones) %>% 
    round(., 3)
  global.rates = map(logs, function(s){
      harmonic.mean(s$`Global Rate`)
  }) %>% unlist()
  
  rates.sdhm = map(logs, function(s){
      harmonic.sd(s$`Global Rate`)
  }) %>% unlist()
  
  
  t = cbind(lh, global.rates, rates.sdhm)
  t = rbind(t, av =  colMeans(t))
  dimnames(t) = list(c("1", "2", "3", "Mean"), c("Marginal log-likelihood", "Harmonic Mean global rate", "Harmonic SD global rate"))
  average_result[[i]] = t
}

names(average_result) = titles

for(i in seq_along(average_result)){
  print(kable(average_result[[i]], caption = titles[i], format = format, digits = 7) %>%
          kable_styling(full_width = FALSE))
  cat('\n')
}

gm = do.call(rbind, gelman_diag)
rownames(gm) = titles
kable(gm, caption = 
        "Table S15: Point estimates and upper 95% confidence limits for Gelman-Rubin MCMC diagnostic tests", 
      format = format, digits = 2) %>%
  kable_styling(full_width = FALSE)

```

## _Example BayesTraits script_
```{bash eval = FALSE, echo = TRUE}
BayesTraitsV3 tree.file data.file 
1
2
NQM
Pis Emp
RevJump exp 10
Stones 100 1000
Iterations 10010000
Burnin 10000
Sample 10000
LogFile logs/file
run
```

## Half-life
We calculate the half-life of each kinterm following methods from @pagel_deep_2018. The half-life of a term estimates the expected amount of time before a 50% chance of a cognate change. 

```{r halflife, echo = FALSE}
sd$half.life = -log(0.5) / (sd$mean.roc / 10000)

sd %>%
  group_by(meaning) %>%
  summarise(mean.hf = mean(half.life) %>% round(0)) %>% 
    kable(col.names = c("Kin code", "Half-life (years)"), format = format,
        caption = "Table S16: Mean half-life for each kin code") %>%
  kable_styling(full_width = FALSE)
```


# Frequency of use and rates of change: Swadesh words and kin terms

```{r }
# combined data
#d = read.csv('data/supplementary_data_Pagel_September.csv')
d = read.csv('data/main-data-si.csv', stringsAsFactors = TRUE)
```

We want to see if 

- Rate of change correlates with frequency of use for kin terms
- What the strength of this relationship is compared to Swadesh terms

The difficulty is that the two data sets are structured differently. A given **kin term** can have a written / spoken / web frequency as well as a word / lemma frequency. A given **Swadesh term** (core vocabulary term) only has one frequency (though it may be written / spoken / etc. depending on the source corpus). Term **length** correlates with frequency of use in a way that is not directly relevant to our analysis either.

In order to create comparable kin- and Swadesh-datasets, we fit a linear mixed model (M1.1) as control on the kinterm data and use word meaning random intercepts from this model in a second, predictive, model (M2)

## Control model: kin terms

>*Model 1.1:*
>
>Centralised log frequency of use per million ~ Corpus genre + Frequency type + (1 | Word meaning)


The aim of M1.1 is to provide us with a word meaning random intercept for M, F, B, Z, etc.\ that incorporates genre and frequency type information. As a result, random slopes were not tested. Table S17 shows the fixed effects for this model.

The word meaning-level random intercepts capture word frequency across data sources. The intercepts predict rate of change, even when controlling for word length. 


```{r M1}
d.k = d %>% dplyr::filter(word.type == 'kin.term') %>% droplevels

# control for genre and frequency type (lemma / word) and word length
m1.1 = lmer(clfpm ~ genre + freq.type + (1|word), data = d.k, REML = F, lmerControl(optimizer = 'bobyqa'))
# summary(m1.1)
m1.1.ranef = data.frame(ranef(m1.1)$word)
m1.1.ranef$word = rownames(m1.1.ranef)
names(m1.1.ranef)[1] = 'frequency.measure'
d.k = merge(d.k,m1.1.ranef)

# print(formula(m1.1))
summary(m1.1)$coef %>%
  kable(format, digits = 2, caption = 'Table S17: Summary of fixed effects for control model 1') %>% 
  kable_styling(full_width = FALSE)
```


```{r figS4, fig.width = 10, fig.height = 8, eval=F}
# ggplot(d.k, aes(x = clfpm, y = frequency.measure, colour = genre, label = word)) +
  # geom_label() + 
  # facet_wrap( ~ freq.type) + 
  # ylab('frequency measure (word intercept)') + 
  # xlab('centralised log frequency per million') +
  # ggtitle('Figure S4: correlation of frequency measure and raw frequencies in control M1')
```



>*Model 1.2:*
>
>Centralised rate of replacement ~ Frequency measure + Word length + (1 | Language)


Since the frequency measure and word length are both word-level predictors, we have no potential random slopes and report the model with a random intercept for language only. The estimates of the fixed effects can be seen in table S18.

M1.1 provides us with aggregated information on the centralised log frequency per million of each kinship word. This allows direct comparison with the core vocabulary (where we only have one datum per word) without a considerable loss of information.

M1.2 only serves to demonstrate that the frequency effect is not an artefact of word length.

```{r M2}
d.k2 = d.k %>% select(word,language,meaning,c.rate,word.type,frequency.measure) %>% unique
d.k2$word %>% 
  as.character %>% 
  nchar %>% 
  as.numeric ->
  d.k2$word.length

m1.2 = lmer(c.rate ~ frequency.measure + word.length + (1 |language), data = d.k2, REML = F, lmerControl(optimizer = 'bobyqa'))
# summary(m1.2)
# print(formula(m1.2))
summary(m1.2)$coef %>%
  kable(format, digits = 2, caption = 'Table S18: Summary of fixed effects for control M2') %>% 
  kable_styling(full_width = FALSE)
```

## Predictive model: rate of change and frequency of use

>*Model 2.1:*
>
>Centralised rate of replacement ~ Frequency measure * Word type + (1 | Language)

>*Model 2.2:*
>
>Centralised rate of replacement ~ Frequency measure * Word type + (Word type | Language)


The predictive model (M2) uses the random word-meaning intercepts (named Frequency measure) from M1.1 as measures of frequency of use for kin terms, and centralised log frequency per million for the Swadesh terms. We restrict the dataset to languages for which we have kin term data. We propose two possible random effect structures, either random intercepts for each language (M2.1), or random slopes for each word type in each language (M2.2. Word-type is the only fixed effect that can vary across language. Goodness-of-fit tests reveal that this random slope results in a better fit (Table S19), so we report M2.2, the model with the slope (Table S20). The interaction effect is plotted in figure S5. Figure S6 shows a plot of the raw data used in the model, highlighting the kinship terms. 

```{r M3}
d.sw = d %>% filter(word.type == 'swadesh.word')
d.sw$frequency.measure = d.sw$clfpm
d.sw$word.length = NA
d.sw2 = d.sw %>% select(word,language,meaning,c.rate,word.type,frequency.measure,word.length)
d2 = rbind(d.k2,d.sw2)

# write.csv(d2, 'data/supplementary_data_processed.csv', row.names = F)

m2.1 = lmer(c.rate ~ frequency.measure * word.type + (word.type|language), data = d2, REML = F, lmerControl(optimizer = 'bobyqa'))

m2.2 = lmer(c.rate ~ frequency.measure * word.type + (1|language), data = d2, REML = F, lmerControl(optimizer = 'bobyqa'))
# summary(m2.1)
# print(formula(m1.2))
anova.object = anova(m2.1,m2.2) %>% 
  tidy
names(anova.object) = names(anova.object)   
anova.object$term = c("M3.2", "M3.1")
anova.object %>% select(term, df, AIC, BIC, logLik, deviance) %>% 
  kable(format, digits = 2, caption = "Table S19: Goodness for fit for model random effect structures") %>% 
  kable_styling(full_width = FALSE)

summary(m2.1)$coef %>%
  kable(format, digits = 2, caption = 'Table S20: Summary of fixed effects for predictive model') %>% 
  kable_styling(full_width = FALSE)
```


```{r fig.width = 5, fig.height = 5}
plot(allEffects(m2.1), multiline = T, ci.style = 'band', lines=list(lty=c(1,2)), main = 'Figure S5: rate of change ~ freq : word type', xlab = 'adjusted log freq per mil, centralised', ylab = 'mean rate of change, centralised')
```

```{r fig.width = 10, fig.height = 8}
d2b = d2 %>% 
  group_by(c.rate,meaning,word.type) %>% 
  summarise(mean.frequency.measure = mean(frequency.measure))

ggplot(d2b, aes(x = mean.frequency.measure, y = c.rate, colour = word.type, label = meaning)) + 
  geom_text(data = d2b[d2b$word.type == 'swadesh.word',], aes(x = mean.frequency.measure, y = c.rate, label = meaning), colour = 'darkcyan') +
  geom_label(data = d2b[d2b$word.type == 'kin.term',], aes(x = mean.frequency.measure, y = c.rate, label = meaning), colour = 'red') +
  xlab('frequency measure') + 
  ylab('rate of change') +
  ggtitle('Figure S6: Mean raw data correlation of rate of change and frequency measure in predictive model')

```


# References